\section{Caracterización de datos de trayectorias individuales}

\noindent El primer paso en el proceso de análisis de datos es la caracterización de los datos. Este proceso implica examinar y comprender la estructura, el contenido y las características de los datos antes de realizar cualquier análisis más profundo. A continuación, se describen las tareas realizadas para caracterizar los datos de trayectorias individuales:
\begin{itemize}
    \item Cargar los datos de trayectorias individuales desde un archivo CSV.
    \item Explorar las primeras filas del conjunto de datos para obtener una visión general de su estructura.
    \item Verificar el número total de registros y columnas en el conjunto de datos.
    \item Identificar y eliminar columnas innecesarias que no aportan valor al análisis.
    \item Identificar y eliminar las filas innecesarias que no aportan valor al análisis.
    \item Identificar y manejar valores faltantes o nulos en el conjunto de datos.

\end{itemize}
\noindent A continuación, se detallan los pasos específicos realizados en el proceso de caracterización:

\subsection{Carga de datos}
\noindent Se cargaron los datos de trayectorias individuales desde un archivo CSV utilizando la biblioteca \texttt{dask}. El archivo contiene información sobre las trayectorias de diferentes individuos, incluyendo coordenadas geográficas y otros atributos relevantes. 

\subsection{Exploración inicial}
\noindent Se exploraron las primeras 2 filas del conjunto de datos para obtener una visión general de su estructura. Esto incluye la identificación de las columnas presentes y un par de registros. Esto se logró gracias al siguiente código:

    \begin{lstlisting}[
        language=Python,
        caption={csv\_glance.py, exploración inicial del conjunto de datos.},
        label={cod:csv_glance}
        ]
        import dask.dataframe as dd

        ruta_archivo = "Mobility_Data.csv"  

        ddf = dd.read_csv(
            ruta_archivo,
            encoding="utf-8",  
            sep=",",          
            dtype="object",    luego)
        )

        columnas = ddf.columns.tolist()

        print("Columnas y 2 ejemplos por cada una:\n")
        for col in columnas:
            ejemplos = ddf[col].head(2).values.tolist() 
            print(f"- {col}: {ejemplos}")
    \end{lstlisting}

\noindent El resultado de la ejecución de este código es el siguiente:
\begin{enumerate}[leftmargin=*, align=left, noitemsep]
    \item \texttt{id}: Identificador numérico único para cada registro ['34284565','34284566'].
    \item \texttt{identifier}: Identificador único del dispositivo 
    \begin{small}
    ['f2640430-7e39-41b7-80bb-3fddaa44779c', 'f2640430-7e39-41b7-80bb-3fddaa44779c'].    
    \end{small}
    
    \item \texttt{identifier\_type}: Tipo de identificador del dispositivo. En este caso, 'gaid' (Google Advertising ID para Android). Otros posibles: 'idfa' (Apple), 'imei' ['gaid', 'gaid'].
    \item \texttt{timestamp}: Fecha y hora del registro de movilida ['2022-11-07 02:04:21', '2022-11-08 17:29:35']
    \item \texttt{device\_lat/device\_lon}: Coordenadas geográficas (latitud y longitud) donde se detectó el dispositivo ['21.843149', '21.843149'], ['-102.196838', '-102.196838'].
    
    \begin{small}
    \item \texttt{country\_short/province\_short}: Código del país (MX = México) y región (MX.01 = Aguascalientes, según estándar ISO) ['MX', 'MX'], ['MX.01', 'MX.01'].
    \item \texttt{ip\_address}: Dirección IP del dispositivo (en formato IPv6)['2806:103e:16::', '2806:103e:16::'].    
    \end{small}
    \item \texttt{device\_horizontal\_accuracy}: Precisión del GPS en metro. Menor valor = mayor precisión ['8.0', '8.0'].
    
    \begin{small}
    \item \texttt{source\_id}: Hash único que identifica la fuente de los datos. Puede ser un identificador de la aplicación o del dispositivo ['449d086de6d9c3d192345c992dfac54319b9d550a92bcd20c37f8368cb428344', '449d086de6d9c3d192345c992dfac54319b9d550a92bcd20c37f8368cb428344'].
    \item \texttt{record\_id}: Identificador único del registro de movilidad (diferente al id) ['77d795df-6972-4f00-ac41-d10d1812bb2d', '8f8e1281-bc4d-4d2c-b00b-eb5c52d75bc1'].
    \item \texttt{home\_country\_code}: País de residencia del usuario ['MX', 'MX'].
    \item \texttt{home\_geog\_point/work\_geog\_point}: Coordenadas geográficas del hogar y del trabajo en formato WKT (Well-Known Text) ['POINT(-102.370380092263 22.2075340951743)', 'POINT(-102.370380092263 22.2075340951743)'].    
    \end{small}
    
    \item \texttt{home\_hex\_id/work\_hex\_id}: Identificador hexadecimal del hogar y del trabajo, representando una ubicación geográfica en un sistema de cuadrícula hexagonal ['85498853fffffff', '85498853fffffff'].
    \item \texttt{data\_execute}: Fecha de procesamiento del registro de movilidad, no necesariamente la fecha de recolección ['2023-05-30', '2023-05-30'].
    \item \texttt{time\_zone\_name}: Zona horaria del dispositivo ['America\textdollar/Mexico\_City', 'America/Mexico\_City'].
\end{enumerate}

\newpage
\subsection{Número de registros y columnas}
\noindent Se verificó el número total de registros y columnas en el conjunto de datos utilizando el siguiente código:

    \begin{lstlisting}[
        language=Python,
        caption={csv\_count\_registers.py, conteo de registros en el conjunto de datos.},
        label={cod:csv_count}
        ]
        import dask.dataframe as dd

        ruta_archivo = "Mobility_Data.csv" 
        columnas_usar = ["id"]  

        ddf = dd.read_csv(
            ruta_archivo,
            usecols=columnas_usar, 
            sep=",",             
            dtype={"id": "str"},  
            blocksize="256MB",   
        )

        print("Contando registros (paciencia para archivos grandes)...")
        total_registros = ddf.shape[0].compute() 

        print(f" Total de registros: {total_registros:,}")
    \end{lstlisting}

\noindent El resultado de la ejecución de este código es que el conjunto de datos contiene un total de 69,980,000 registros y 19 campos. Esto indica que hay una cantidad significativa de datos disponibles para el análisis.

\subsection{Identificar y eliminar campos innecesarios que no aportan valor al análisis}
\noindent Ya que tenemmos un conjunto de datos con 19 campos, es importante identificar y eliminar aquellas que no aportan valor al análisis. Para ello, vamos a revisar los valores únicos de los siguiente campos, para determinar si son redundantes o no aportan información relevante. A continuación, se presentan los campos que se consideran innecesarios:
\begin{itemize}
    \item \texttt{id}
    \item \texttt{identifier\_type}
    \item \texttt{country\_short}
    \item \texttt{province\_short}
    \item \texttt{ip\_address}
    \item \texttt{source\_id}
    \item \texttt{home\_country\_code}
    \item \texttt{home\_geog\_point}
    \item \texttt{work\_geog\_point}
    \item \texttt{home\_hex\_id}
    \item \texttt{work\_hex\_id}
    \item \texttt{data\_execute}
\end{itemize}

\noindent Para eliminar estos campos vamos a tomar solamente las columnas que sí vamos a conservar. A continuación, se muestra el código utilizado para realizar esta tarea:

    \begin{lstlisting}[
        language=Python,
        caption={remove\_columns.py, eliminación de campos innecesarios en el conjunto de datos.},
        label={cod:csv_slim}
        ]
        import dask.dataframe as dd

        # Columnas que si vamos a conservar
        columnas_deseadas = [
            'identifier',
            'timestamp',
            'device_lat',
            'device_lon',
            'device_horizontal_accuracy',
            'record_id',
            'time_zone_name'
            ]

        # Cargar solo las columnas necesarias
        df = dd.read_csv('Mobility_Data.csv', usecols=columnas_deseadas)

        # Guardar el resultado
        df.to_csv('Mobility_Data_Slim.csv', index=False, single_file=True, encoding='utf-8-sig')
    \end{lstlisting}

\noindent El resultado de la ejecución de este código es un nuevo archivo CSV que se guarda en la misma carpeta el cual contiene únicamente las columnas seleccionadas, eliminando así las que no aportan valor al análisis. 

\newpage
\subsection{Identificar y eliminar las filas innecesarias que no aportan valor al análisis}
\noindent Ahora que tenemos una base datos más ligera, es importante identificar y eliminar las filas que no aportan valor al análisis. Para ellos, vamos a realizar diagramas que nos permitan identificar la distribución de los datos y poder determinar si hay filas que no aportan valor al análisis. Las tablas seleccionadas para este análisis son las siguientes:
\begin{itemize}
    \item \texttt{identifier}: Identificador único del dispositivo.
    \item \texttt{device\_horizontal\_accuracy}: Precisión del GPS en metro. Menor valor = mayor precisión. 
\end{itemize}

\noindent La primer columna analizada fue \texttt{device\_horizontal\_accuracy}, que representa la precisión del GPS en metros. Estos valores dependen del sistema de medición y la fuente de los datos, generalemente se sigue la siguiente escala:
\begin{itemize}
    \item GPS puro (satelital): 1-20 metros.
    \item A-GPS (Asistido por red): 5-50 metros.
    \item Triangulación por WiFi/redes móviles: 20-500 metros.
    \item Geolocalización por IP: 1000-5000 metros.
\end{itemize}

\noindent Con base en esta escala, el primer paso es identificar el rango de valores que contiene la columna. Para ello, se utilizó el \textit{Código:} \ref{cod:unique_values}, que permite obtener los valores únicos de la columna \texttt{device\_horizontal\_accuracy} y guardarlos en un archivo de texto. 

    \begin{lstlisting}[
        language=Python,
        breaklines=true,
        caption={unique\_values.py, obtención de valores únicos de la columna 'device\_horizontal\_accuracy'.},
        label={cod:unique_values}
        ]
        import dask.dataframe as dd
        import pandas as pd
        from tqdm import tqdm

        archivo_csv = "Mobility_Data.csv" 
        columna_objetivo = "device_horizontal_accuracy"  
        archivo_salida = "valores_unicos.txt"
        chunksize = 1_000_000  # Procesar 1M de registros a la vez

        valores_unicos = set()

        for chunk in tqdm(pd.read_csv(archivo_csv, usecols=[columna_objetivo], chunksize=chunksize)):
            valores_unicos.update(chunk[columna_objetivo].dropna().astype(str)) 

    
        with open(archivo_salida, "w", encoding="utf-8") as f:
            f.write("\n".join(sorted(valores_unicos)))  

        print(f"\n Se encontraron {len(valores_unicos):,} valores unicos.")
        print(f" Guardados en: {archivo_salida}")

        print("\n Ejemplo de valores unicos:")
        print("\n".join(sorted(valores_unicos)[:10]))
    \end{lstlisting}

\noindent Una vez obtenidos los valores únicos pudimos determinar que el rango de valores es de 0 a 200. Este rango es muy importante para poder crear un script que grafique los valores de la columna \texttt{device\_horizontal\_accuracy} y que nos permita identificar la frecuencia de los valores para poder determinar si hay filas que no aportan valor al análisis. Este script se muestra en el \textit{Código:} \ref{cod:accuracy_histogram}.

    \begin{lstlisting}[
        language=Python,
        caption={accuracy\_histogram.py, creación de un histograma de frecuencias de la columna 'device\_horizontal\_accuracy'.},
        label={cod:accuracy_histogram}
        ]
        import os
        import pandas as pd
        import matplotlib.pyplot as plt
        import numpy as np

        archivo_csv = "Mobility_Data_Slim.csv"
        columna = "device_horizontal_accuracy"  
        bins = 100 
        os.makedirs("img", exist_ok=True) 

        frecuencias = pd.Series(dtype=float)
        for chunk in pd.read_csv(archivo_csv, usecols=[columna], chunksize=1_000_000):
            frecuencias = pd.concat([frecuencias, chunk[columna].value_counts()])

        counts, edges = np.histogram(frecuencias.index, bins=bins, weights=frecuencias.values)

        plt.figure(figsize=(12, 6))
        plt.bar(edges[:-1], counts, width=np.diff(edges), align='edge', edgecolor='black', alpha=0.7)
        plt.title("Frecuencias de Valores Agrupados por Rangos (0-200)")
        plt.xlabel("Rango de Valores")
        plt.ylabel("Frecuencia Total (Millones)")
        plt.xticks(edges[::5], rotation=45) 
        plt.grid(axis='y', linestyle='--')

        output_path = "img/histograma_frecuencias_rangos.png"
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()
    \end{lstlisting}

\noindent El resultado de la ejecución del script anterior es un histograma que muestra la frecuencia de los valores agrupados por rangos. En la siguiente figure se puede observar que la mayoría de los valores se encuentran en el rango de 0 a 20 metros, lo cual es consistente con la precisión del GPS puro. Con base en esto podemos acotar los valores que serán considerados válidos para el análisis.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{img/histograma_frecuencias_rangos.png}
    \caption{Frecuencias de valores agrupados por rangos (0-200).}
    \label{fig:accuracy_histogram}
\end{figure}

\noindent La siguiente columna analizada fue \texttt{identifier}, que representa el identificador único del dispositivo. Para esta columna, se realizó un análisis de los valores únicos para determinar si hay dispositivos que no aportan valor al análisis. El \textit{Código:} \ref{cod:unique_identifiers} muestra el código utilizado para obtener los valores únicos de la columna \texttt{identifier} y guardarlos en un archivo de texto.


\subsection{Identificar y manejar valores faltantes o nulos en el conjunto de datos}

prueba de push